{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063f7c1a-db78-4f72-9e47-1f0d11c61a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import td_load_data\n",
    "import td_run_model2\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a47281-bab2-4a5d-b712-be12eaf461de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bb1a0f-c109-41ca-9a1a-490ed65cede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3  # Adjust this to match the number of classes (e.g., high, medium, low)\n",
    "num_epochs = 23\n",
    "batch_size = 16\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586cd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16MultiClassClassifier, self).__init__()\n",
    "        self.base_model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        in_features = self.base_model.classifier[6].in_features\n",
    "        \n",
    "        self.base_model.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794688ea-4590-4d4d-898a-9bb98927df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "\n",
    "    # Model initialization\n",
    "    model = VGG16MultiClassClassifier(num_classes=num_classes).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load data\n",
    "    train_loader, validation_loader, test_loader, classes = td_load_data.create_data(batch_size=batch_size)\n",
    "    \n",
    "    # Test the model's forward pass with a sample input\n",
    "    sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    print(\"Output shape:\", model(sample_input).shape)  # This should now be [1, num_classes]\n",
    "    \n",
    "    # Train and validate the model\n",
    "    td_run_model2.train(num_epochs, device, model, criterion, optimizer, train_loader, validation_loader, num_classes)\n",
    "    \n",
    "    \n",
    "    # Test the model\n",
    "    td_run_model2.test(device, model, test_loader, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea11b622-cd7c-4f06-abdb-c65898103ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abiya\\AppData\\Local\\Temp\\ipykernel_23264\\571911711.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('weights/43.pth', map_location=torch.device('cpu'))     # Running locally\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in C:/dDrive/CS480-fyp/traffic_density/one day/1501.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the test data using the function in td_load_data.py\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m _, _, test_loader, classes \u001b[38;5;241m=\u001b[39m \u001b[43mtd_load_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     18\u001b[0m td_run_model2\u001b[38;5;241m.\u001b[39mtest_model_on_test_data(device, model, test_loader)\n",
      "File \u001b[1;32mc:\\dDrive\\CS480-fyp\\fyp code\\CAM-R\\models\\traffic_density\\td_load_data.py:22\u001b[0m, in \u001b[0;36mcreate_data\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39mtrain_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     21\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39mval_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m---> 22\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Create DataLoaders\u001b[39;00m\n\u001b[0;32m     25\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Abiya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\Abiya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\Abiya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Abiya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in C:/dDrive/CS480-fyp/traffic_density/one day/1501."
     ]
    }
   ],
   "source": [
    "# Define the model architecture with the same number of classes used during training\n",
    "model = VGG16MultiClassClassifier(num_classes=num_classes).to(device)\n",
    "\n",
    "# Load the state dict\n",
    "# state_dict = torch.load('weights/43.pth')\n",
    "# state_dict = torch.load('weights/42.pth')\n",
    "state_dict = torch.load('weights/43.pth', map_location=torch.device('cpu'))     # Running locally\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the test data using the function in td_load_data.py\n",
    "_, _, test_loader, classes = td_load_data.create_data(batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "td_run_model2.test_model_on_test_data(device, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a81ee25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abiya\\AppData\\Local\\Temp\\ipykernel_14036\\2480954960.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('weights/43.pth', map_location=torch.device('cpu')))  # Path to your weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos have been successfully sorted into high, medium, and low folders.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "input_folder = \"C:/dDrive/CS480-fyp/traffic_density/8702\"  # Path to the folder with unsorted photos\n",
    "output_folder = \"sorted_photos/\"   # Base path to save sorted photos\n",
    "high_folder = os.path.join(output_folder, \"high\")\n",
    "medium_folder = os.path.join(output_folder, \"medium\")\n",
    "low_folder = os.path.join(output_folder, \"low\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(high_folder, exist_ok=True)\n",
    "os.makedirs(medium_folder, exist_ok=True)\n",
    "os.makedirs(low_folder, exist_ok=True)\n",
    "\n",
    "# Load the VGG16 model (replace this if your model is defined differently in vgg16.ipynb)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VGG16MultiClassClassifier(num_classes=num_classes).to(device)  # Change this based on your specific model\n",
    "#model.classifier[6] = torch.nn.Linear(4096, 3)  # Assuming you're classifying into 3 classes\n",
    "#state_dict = torch.load('weights/43.pth', map_location=torch.device('cpu'))\n",
    "#model.load_state_dict(state_dict)\n",
    "model.load_state_dict(torch.load('weights/43.pth', map_location=torch.device('cpu')))  # Path to your weights\n",
    "# Load the saved model state dict\n",
    "# state_dict = torch.load('weights/43.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# # Remove 'base_model.' prefix from the keys in the state_dict\n",
    "# new_state_dict = {}\n",
    "# for key in state_dict.keys():\n",
    "#     new_key = key.replace(\"base_model.\", \"\")  # Adjust this based on your model architecture\n",
    "#     new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "# # Load the modified state dict into the model\n",
    "# model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the transformation (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to predict class for a single image\n",
    "def predict_image(image_path, model, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return predicted.item()  # Return the predicted class index (0, 1, 2 for high, medium, low)\n",
    "\n",
    "# Loop through each image in the input folder\n",
    "for img_name in os.listdir(input_folder):\n",
    "    img_path = os.path.join(input_folder, img_name)\n",
    "    if os.path.isfile(img_path):\n",
    "        predicted_class = predict_image(img_path, model, device)\n",
    "        \n",
    "        # Move image to the corresponding class folder\n",
    "        if predicted_class == 0:  # Assuming 0 is high, 1 is medium, 2 is low\n",
    "            shutil.move(img_path, os.path.join(high_folder, img_name))\n",
    "        elif predicted_class == 2:\n",
    "            shutil.move(img_path, os.path.join(medium_folder, img_name))\n",
    "        elif predicted_class == 1:\n",
    "            shutil.move(img_path, os.path.join(low_folder, img_name))\n",
    "\n",
    "print(\"Photos have been successfully sorted into high, medium, and low folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5e85d-9005-4631-8c6d-24b7462ff29c",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
